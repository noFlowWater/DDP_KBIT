{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP_KBIT Jupyter Notebook Interface\n",
    "\n",
    "This notebook provides a simple interface to run the DDP_KBIT distributed deep learning system without using command line arguments. It wraps the existing `main.py` functionality for easy experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error importing DDP_KBIT modules: No module named 'main'\n",
      "Please ensure you're running from the correct directory and all dependencies are installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# Add the current directory to the Python path for imports\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Import the main functions from main.py\n",
    "try:\n",
    "    from main import (\n",
    "        setup_logging, \n",
    "        load_external_config,\n",
    "        run_training_mode,\n",
    "        run_experiment_mode, \n",
    "        create_sample_config\n",
    "    )\n",
    "    print(\"‚úì Successfully imported DDP_KBIT modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing DDP_KBIT modules: {e}\")\n",
    "    print(\"Please ensure you're running from the correct directory and all dependencies are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Setup logging\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msetup_logging\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a mock args object to simulate command line arguments\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNotebookArgs\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_logging' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "setup_logging(\"INFO\")\n",
    "\n",
    "# Create a mock args object to simulate command line arguments\n",
    "class NotebookArgs:\n",
    "    def __init__(self):\n",
    "        self.config_path = \"sample_config.json\"\n",
    "        self.distributed = False\n",
    "        self.experiment_type = \"single\"\n",
    "        self.iterations = 3\n",
    "        self.log_level = \"INFO\"\n",
    "\n",
    "# Initialize default arguments\n",
    "args = NotebookArgs()\n",
    "\n",
    "print(\"‚úì Configuration setup complete\")\n",
    "print(f\"Config path: {args.config_path}\")\n",
    "print(f\"Distributed: {args.distributed}\")\n",
    "print(f\"Iterations: {args.iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Configuration (Run this first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample configuration file\n",
    "create_sample_config()\n",
    "print(\"‚úì Sample configuration created!\")\n",
    "\n",
    "# Display the configuration\n",
    "if os.path.exists(\"sample_config.json\"):\n",
    "    with open(\"sample_config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"\\nCurrent configuration:\")\n",
    "    print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode\n",
    "\n",
    "Run single node or distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single node training\n",
    "print(\"üöÄ Starting single node training...\")\n",
    "args.distributed = False\n",
    "\n",
    "try:\n",
    "    run_training_mode(args)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training (uncomment to run)\n",
    "# print(\"üöÄ Starting distributed training...\")\n",
    "# args.distributed = True\n",
    "\n",
    "# try:\n",
    "#     run_training_mode(args)\n",
    "#     print(\"‚úÖ Distributed training completed successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Distributed training failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Mode\n",
    "\n",
    "Run single experiments or multiple iterations with statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single experiment\n",
    "print(\"üß™ Running single experiment...\")\n",
    "args.experiment_type = \"single\"\n",
    "\n",
    "try:\n",
    "    run_experiment_mode(args)\n",
    "    print(\"‚úÖ Single experiment completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Single experiment failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple experiments with statistical analysis\n",
    "print(\"üß™ Running multiple experiments...\")\n",
    "args.experiment_type = \"multiple\"\n",
    "args.iterations = 5  # You can change this number\n",
    "\n",
    "try:\n",
    "    run_experiment_mode(args)\n",
    "    print(f\"‚úÖ {args.iterations} experiments completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Multiple experiments failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Mode\n",
    "\n",
    "Run performance benchmarking with multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "print(\"‚ö° Running performance benchmarks...\")\n",
    "args.iterations = 5  # You can change this number\n",
    "\n",
    "try:\n",
    "    run_benchmark_mode(args)\n",
    "    print(f\"‚úÖ Benchmark with {args.iterations} iterations completed successfully!\")\n",
    "    \n",
    "    # Show if results file was created\n",
    "    results_file = f\"benchmark_results_{args.iterations}_iterations.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        print(f\"üìä Results saved to: {results_file}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Benchmarking failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Configuration\n",
    "\n",
    "Modify configuration parameters for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize configuration\n",
    "custom_config = {\n",
    "    \"spark_config\": {\n",
    "        \"master\": \"local[*]\",\n",
    "        \"app_name\": \"DDP_KBIT_Custom\",\n",
    "        \"executor_instances\": 4,\n",
    "       