{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP_KBIT Jupyter Notebook Interface\n",
    "\n",
    "This notebook provides a simple interface to run the DDP_KBIT distributed deep learning system without using command line arguments. It wraps the existing `main.py` functionality for easy experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세션 초기화 (매번 실행 필요)\n",
    "\n",
    "아래 셀을 매 세션마다 가장 먼저 실행하여 로컬 모듈에 연결하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found DDP_KBIT at: /mnt/data/DDP_KBIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: config.json not found. Using default values.\n",
      "✓ Successfully imported DDP_KBIT modules directly\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# DDP_KBIT 모듈 경로 설정 - 현재 디렉토리 또는 상위 디렉토리에서 찾기\n",
    "current_dir = os.getcwd()\n",
    "possible_paths = [\n",
    "    current_dir,  # 현재 디렉토리\n",
    "    os.path.dirname(current_dir),  # 상위 디렉토리\n",
    "    r\"/mnt/data/DDP_KBIT\",  # Jupyter 환경\n",
    "    r\"D:\\Nextcloud3\\kbit\\DDP_KBIT\",  # Windows 로컬 환경\n",
    "]\n",
    "\n",
    "ddp_kbit_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(os.path.join(path, \"main.py\")):\n",
    "        ddp_kbit_path = path\n",
    "        break\n",
    "\n",
    "if ddp_kbit_path:\n",
    "    if ddp_kbit_path not in sys.path:\n",
    "        sys.path.insert(0, ddp_kbit_path)\n",
    "    print(f\"✓ Found DDP_KBIT at: {ddp_kbit_path}\")\n",
    "else:\n",
    "    print(\"❌ Could not find DDP_KBIT directory\")\n",
    "\n",
    "# Import the main functions from main.py\n",
    "try:\n",
    "    # 패키지로 임포트 시도\n",
    "    try:\n",
    "        import DDP_KBIT\n",
    "        from DDP_KBIT.main import (\n",
    "            setup_logging, \n",
    "            load_external_config,\n",
    "            run_training_mode,\n",
    "            run_experiment_mode, \n",
    "            create_sample_config\n",
    "        )\n",
    "        print(\"✓ Successfully imported DDP_KBIT as package\")\n",
    "    except ImportError:\n",
    "        # 직접 임포트 시도\n",
    "        from main import (\n",
    "            setup_logging, \n",
    "            load_external_config,\n",
    "            run_training_mode,\n",
    "            run_experiment_mode, \n",
    "            create_sample_config\n",
    "        )\n",
    "        print(\"✓ Successfully imported DDP_KBIT modules directly\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing DDP_KBIT modules: {e}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(\"Available Python files:\")\n",
    "    try:\n",
    "        import glob\n",
    "        py_files = glob.glob(\"*.py\")\n",
    "        if py_files:\n",
    "            print(f\"  Python files found: {py_files}\")\n",
    "        else:\n",
    "            print(\"  No Python files found in current directory\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 대안: 절대 경로로 main.py 실행\n",
    "    if ddp_kbit_path and os.path.exists(os.path.join(ddp_kbit_path, \"main.py\")):\n",
    "        print(\"\\nTrying to execute main.py directly...\")\n",
    "        try:\n",
    "            exec(open(os.path.join(ddp_kbit_path, \"main.py\")).read(), globals())\n",
    "            print(\"✓ Successfully loaded main.py using exec method\")\n",
    "        except Exception as exec_error:\n",
    "            print(f\"❌ Exec method failed: {exec_error}\")\n",
    "    else:\n",
    "        print(\"❌ main.py not found in any expected location\")\n",
    "        print(\"Please ensure you're running from the correct directory and all dependencies are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration setup complete\n",
      "Config path: sample_config.json\n",
      "Distributed: False\n",
      "Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "setup_logging(\"INFO\")\n",
    "\n",
    "# Create a mock args object to simulate command line arguments\n",
    "class NotebookArgs:\n",
    "    def __init__(self):\n",
    "        self.config_path = \"sample_config.json\"\n",
    "        self.distributed = False\n",
    "        self.experiment_type = \"single\"\n",
    "        self.iterations = 3\n",
    "        self.log_level = \"INFO\"\n",
    "\n",
    "# Initialize default arguments\n",
    "args = NotebookArgs()\n",
    "\n",
    "print(\"✓ Configuration setup complete\")\n",
    "print(f\"Config path: {args.config_path}\")\n",
    "print(f\"Distributed: {args.distributed}\")\n",
    "print(f\"Iterations: {args.iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Configuration (Run this first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample_config.json - customize this file for your needs.\n",
      "✓ Sample configuration created!\n",
      "\n",
      "Current configuration:\n",
      "{\n",
      "  \"training_config\": {\n",
      "    \"base_model_type\": \"NeuralNetwork\",\n",
      "    \"optimizer_class\": \"torch.optim.adam.Adam\",\n",
      "    \"optimizer_params\": {\n",
      "      \"lr\": 0.001\n",
      "    },\n",
      "    \"loss_fn\": \"torch.nn.modules.loss.CrossEntropyLoss\",\n",
      "    \"perform_validation\": true,\n",
      "    \"num_epochs\": 1,\n",
      "    \"batch_size\": 32,\n",
      "    \"metrics\": {\n",
      "      \"loss\": \"Loss\",\n",
      "      \"accuracy\": \"Accuracy\"\n",
      "    }\n",
      "  },\n",
      "  \"mongo_config\": {\n",
      "    \"connection_id\": \"my-mongo-1\",\n",
      "    \"mongo_database\": \"kbit-db\",\n",
      "    \"collection\": \"mnist_train_avro\"\n",
      "  },\n",
      "  \"kafka_config\": {\n",
      "    \"bootstrap_servers\": [\n",
      "      \"155.230.35.200:32100\",\n",
      "      \"155.230.35.213:32100\",\n",
      "      \"155.230.35.215:32100\"\n",
      "    ],\n",
      "    \"data_load_topic\": \"kbit-p3r1\"\n",
      "  },\n",
      "  \"data_loader_config\": {\n",
      "    \"data_loader_type\": \"kafka\",\n",
      "    \"local_data_path\": \"/root/processed_mnist\",\n",
      "    \"offsets_data\": [\n",
      "      \"0:0:19999\",\n",
      "      \"1:0:19999\",\n",
      "      \"2:0:19999\"\n",
      "    ],\n",
      "    \"offsets_data_topic\": \"my-topic-3\",\n",
      "    \"api_config\": {\n",
      "      \"base_url\": \"http://155.230.36.25:3001\",\n",
      "      \"endpoint\": \"data/export\",\n",
      "      \"params\": {\n",
      "        \"connection_id\": \"my-mongo-1\",\n",
      "        \"mongo_database\": \"kbit-db\",\n",
      "        \"collection\": \"mnist_train_avro\",\n",
      "        \"kafka_brokers\": \"155.230.35.200:32100,155.230.35.213:32100,155.230.35.215:32100\",\n",
      "        \"send_topic\": \"kbit-p3r1\"\n",
      "      }\n",
      "    },\n",
      "    \"dataset_split_config\": [\n",
      "      {\n",
      "        \"rate\": 0.85715\n",
      "      },\n",
      "      {\n",
      "        \"rate\": 0.071425\n",
      "      },\n",
      "      {\n",
      "        \"rate\": 0.071425\n",
      "      }\n",
      "    ],\n",
      "    \"consumer_params\": {\n",
      "      \"bootstrap_servers\": [\n",
      "        \"155.230.35.200:32100\",\n",
      "        \"155.230.35.213:32100\",\n",
      "        \"155.230.35.215:32100\"\n",
      "      ]\n",
      "    },\n",
      "    \"payload_config\": {\n",
      "      \"message_format\": \"none\",\n",
      "      \"data_field\": \"data\",\n",
      "      \"label_field\": \"label\",\n",
      "      \"transform_data_fn\": \"transform_mongodb_image\",\n",
      "      \"transform_label_fn\": null\n",
      "    }\n",
      "  },\n",
      "  \"payload_config\": {\n",
      "    \"message_format\": \"none\",\n",
      "    \"data_field\": \"data\",\n",
      "    \"label_field\": \"label\",\n",
      "    \"transform_data_fn\": \"transform_mongodb_image\",\n",
      "    \"transform_label_fn\": null\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"generated_from\": \"DDP_KBIT.config modules\",\n",
      "    \"description\": \"Sample configuration dynamically generated from existing config modules\",\n",
      "    \"usage_note\": \"This config uses the same settings as defined in the original notebook cell 24\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a sample configuration file\n",
    "create_sample_config()\n",
    "print(\"✓ Sample configuration created!\")\n",
    "\n",
    "# Display the configuration\n",
    "if os.path.exists(\"sample_config.json\"):\n",
    "    with open(\"sample_config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"\\nCurrent configuration:\")\n",
    "    print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode\n",
    "\n",
    "Run single node or distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 05:25:25 - root - INFO - Starting training mode...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting single node training...\n",
      "FILES IN THIS DIRECTORY\n",
      "['DDP_KBIT', 'jars', 'config.json', 'mnist_pb2.py', 'spark_DL_checkpoints']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/20 05:25:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/20 05:25:27 WARN ResourceUtils: The configuration of cores (exec = 4 task = 1, runnable tasks = 4) will result in wasted resources due to resource gpu limiting the number of runnable tasks per executor to: 1. Please adjust your configuration.\n",
      "25/08/20 05:25:27 WARN Utils: Service 'sparkDriver' could not bind on port 39337. Attempting port 39338.\n",
      "25/08/20 05:25:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/20 05:25:27 WARN RapidsPluginUtils: RAPIDS Accelerator 24.06.1 using cudf 24.06.0, private revision 755b4dd03c753cacb7d141f3b3c8ff9f83888b69\n",
      "25/08/20 05:25:27 WARN RapidsPluginUtils: spark.rapids.sql.multiThreadedRead.numThreads is set to 20.\n",
      "25/08/20 05:25:27 WARN RapidsPluginUtils: The current setting of spark.task.resource.gpu.amount (1.0) is not ideal to get the best performance from the RAPIDS Accelerator plugin. It's recommended to be 1/{executor core count} unless you have a special use case.\n",
      "25/08/20 05:25:27 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "25/08/20 05:25:27 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `NOT_ON_GPU`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n",
      "2025-08-20 05:25:28 - root - ERROR - Training failed: main_fn() missing 3 required positional arguments: 'training_config', 'kafka_config', and 'data_loader_config'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Spark configuration:\n",
      "spark.app.id = app-20250820052528-0019\n",
      "spark.app.initial.file.urls = spark://192.168.141.56:39338/files/mnist_pb2.py\n",
      "spark.app.initial.jar.urls = spark://192.168.141.56:39338/jars/hadoop-client-runtime-3.3.4.jar,spark://192.168.141.56:39338/jars/kafka-clients-3.4.1.jar,spark://192.168.141.56:39338/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/jsr305-3.0.0.jar,spark://192.168.141.56:39338/jars/rapids-4-spark_2.12-24.06.1.jar,spark://192.168.141.56:39338/jars/hadoop-client-api-3.3.4.jar,spark://192.168.141.56:39338/jars/slf4j-api-2.0.7.jar,spark://192.168.141.56:39338/jars/commons-pool2-2.11.1.jar,spark://192.168.141.56:39338/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/commons-logging-1.1.3.jar,spark://192.168.141.56:39338/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/lz4-java-1.8.0.jar,spark://192.168.141.56:39338/jars/snappy-java-1.1.10.3.jar\n",
      "spark.app.name = DDP_KBIT_Training\n",
      "spark.app.startTime = 1755667527043\n",
      "spark.app.submitTime = 1755667526928\n",
      "spark.default.parallelism = 64\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host = 192.168.141.56\n",
      "spark.driver.port = 39338\n",
      "spark.executor.cores = 4\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id = driver\n",
      "spark.executor.instances = 8\n",
      "spark.executor.memory = 6G\n",
      "spark.executor.resource.gpu.amount = 1\n",
      "spark.executor.resource.gpu.discoveryScript = /opt/spark/conf/getGpusResources.sh\n",
      "spark.files = file:///mnt/data/mnist_pb2.py\n",
      "spark.jars = jars/commons-logging-1.1.3.jar,jars/commons-pool2-2.11.1.jar,jars/hadoop-client-api-3.3.4.jar,jars/hadoop-client-runtime-3.3.4.jar,jars/jsr305-3.0.0.jar,jars/kafka-clients-3.4.1.jar,jars/lz4-java-1.8.0.jar,jars/slf4j-api-2.0.7.jar,jars/snappy-java-1.1.10.3.jar,jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.master = spark://spark-master-service:7077\n",
      "spark.plugins = com.nvidia.spark.SQLPlugin\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.driver.user.timezone = Z\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rapids.driver.user.timezone = Z\n",
      "spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rdd.compress = True\n",
      "spark.repl.local.jars = file:///mnt/data/jars/commons-logging-1.1.3.jar,file:///mnt/data/jars/commons-pool2-2.11.1.jar,file:///mnt/data/jars/hadoop-client-api-3.3.4.jar,file:///mnt/data/jars/hadoop-client-runtime-3.3.4.jar,file:///mnt/data/jars/jsr305-3.0.0.jar,file:///mnt/data/jars/kafka-clients-3.4.1.jar,file:///mnt/data/jars/lz4-java-1.8.0.jar,file:///mnt/data/jars/slf4j-api-2.0.7.jar,file:///mnt/data/jars/snappy-java-1.1.10.3.jar,file:///mnt/data/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.sql.extensions = com.nvidia.spark.rapids.SQLExecPlugin,com.nvidia.spark.udf.Plugin,com.nvidia.spark.rapids.optimizer.SQLOptimizerPlugin\n",
      "spark.sql.shuffle.partitions = 64\n",
      "spark.submit.deployMode = client\n",
      "spark.submit.pyFiles = mnist_pb2.py\n",
      "spark.task.resource.gpu.amount = 1\n",
      "spark.ui.showConsoleProgress = true\n",
      "❌ Training failed: main_fn() missing 3 required positional arguments: 'training_config', 'kafka_config', and 'data_loader_config'\n"
     ]
    }
   ],
   "source": [
    "# Single node training\n",
    "print(\"🚀 Starting single node training...\")\n",
    "args.distributed = False\n",
    "\n",
    "try:\n",
    "    run_training_mode(args)\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training (uncomment to run)\n",
    "# print(\"🚀 Starting distributed training...\")\n",
    "# args.distributed = True\n",
    "\n",
    "# try:\n",
    "#     run_training_mode(args)\n",
    "#     print(\"✅ Distributed training completed successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Distributed training failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Mode\n",
    "\n",
    "Run single experiments or multiple iterations with statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 05:25:28 - root - INFO - Starting experiment mode: single\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Running single experiment...\n",
      "FILES IN THIS DIRECTORY\n",
      "['DDP_KBIT', 'jars', 'config.json', 'mnist_pb2.py', 'spark_DL_checkpoints']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 05:25:29 - root - ERROR - Experiments failed: exp_fn() missing 3 required positional arguments: 'training_config', 'kafka_config', and 'data_loader_config'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Spark configuration:\n",
      "spark.app.id = app-20250820052529-0020\n",
      "spark.app.initial.file.urls = spark://192.168.141.56:39338/files/mnist_pb2.py\n",
      "spark.app.initial.jar.urls = spark://192.168.141.56:39338/jars/hadoop-client-runtime-3.3.4.jar,spark://192.168.141.56:39338/jars/kafka-clients-3.4.1.jar,spark://192.168.141.56:39338/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/jsr305-3.0.0.jar,spark://192.168.141.56:39338/jars/rapids-4-spark_2.12-24.06.1.jar,spark://192.168.141.56:39338/jars/hadoop-client-api-3.3.4.jar,spark://192.168.141.56:39338/jars/slf4j-api-2.0.7.jar,spark://192.168.141.56:39338/jars/commons-pool2-2.11.1.jar,spark://192.168.141.56:39338/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/commons-logging-1.1.3.jar,spark://192.168.141.56:39338/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/lz4-java-1.8.0.jar,spark://192.168.141.56:39338/jars/snappy-java-1.1.10.3.jar\n",
      "spark.app.name = DDP_KBIT_Experiments\n",
      "spark.app.startTime = 1755667528985\n",
      "spark.app.submitTime = 1755667526928\n",
      "spark.default.parallelism = 64\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host = 192.168.141.56\n",
      "spark.driver.port = 39338\n",
      "spark.executor.cores = 4\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id = driver\n",
      "spark.executor.instances = 8\n",
      "spark.executor.memory = 6G\n",
      "spark.executor.resource.gpu.amount = 1\n",
      "spark.executor.resource.gpu.discoveryScript = /opt/spark/conf/getGpusResources.sh\n",
      "spark.files = file:///mnt/data/mnist_pb2.py\n",
      "spark.jars = jars/commons-logging-1.1.3.jar,jars/commons-pool2-2.11.1.jar,jars/hadoop-client-api-3.3.4.jar,jars/hadoop-client-runtime-3.3.4.jar,jars/jsr305-3.0.0.jar,jars/kafka-clients-3.4.1.jar,jars/lz4-java-1.8.0.jar,jars/slf4j-api-2.0.7.jar,jars/snappy-java-1.1.10.3.jar,jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.master = spark://spark-master-service:7077\n",
      "spark.plugins = com.nvidia.spark.SQLPlugin\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.driver.user.timezone = Z\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rapids.driver.user.timezone = Z\n",
      "spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rdd.compress = True\n",
      "spark.repl.local.jars = file:///mnt/data/jars/commons-logging-1.1.3.jar,file:///mnt/data/jars/commons-pool2-2.11.1.jar,file:///mnt/data/jars/hadoop-client-api-3.3.4.jar,file:///mnt/data/jars/hadoop-client-runtime-3.3.4.jar,file:///mnt/data/jars/jsr305-3.0.0.jar,file:///mnt/data/jars/kafka-clients-3.4.1.jar,file:///mnt/data/jars/lz4-java-1.8.0.jar,file:///mnt/data/jars/slf4j-api-2.0.7.jar,file:///mnt/data/jars/snappy-java-1.1.10.3.jar,file:///mnt/data/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.sql.extensions = com.nvidia.spark.rapids.SQLExecPlugin,com.nvidia.spark.udf.Plugin,com.nvidia.spark.rapids.optimizer.SQLOptimizerPlugin\n",
      "spark.sql.shuffle.partitions = 64\n",
      "spark.submit.deployMode = client\n",
      "spark.submit.pyFiles = mnist_pb2.py\n",
      "spark.task.resource.gpu.amount = 1\n",
      "spark.ui.showConsoleProgress = true\n",
      "❌ Single experiment failed: exp_fn() missing 3 required positional arguments: 'training_config', 'kafka_config', and 'data_loader_config'\n"
     ]
    }
   ],
   "source": [
    "# Single experiment\n",
    "print(\"🧪 Running single experiment...\")\n",
    "args.experiment_type = \"single\"\n",
    "\n",
    "try:\n",
    "    run_experiment_mode(args)\n",
    "    print(\"✅ Single experiment completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Single experiment failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 05:25:29 - root - INFO - Starting experiment mode: multiple\n",
      "2025-08-20 05:25:29 - root - ERROR - Experiments failed: run_multiple_experiments() got an unexpected keyword argument 'iterations'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Running multiple experiments...\n",
      "FILES IN THIS DIRECTORY\n",
      "['DDP_KBIT', 'jars', 'config.json', 'mnist_pb2.py', 'spark_DL_checkpoints']\n",
      "Current Spark configuration:\n",
      "spark.app.id = app-20250820052529-0021\n",
      "spark.app.initial.file.urls = spark://192.168.141.56:39338/files/mnist_pb2.py\n",
      "spark.app.initial.jar.urls = spark://192.168.141.56:39338/jars/hadoop-client-runtime-3.3.4.jar,spark://192.168.141.56:39338/jars/kafka-clients-3.4.1.jar,spark://192.168.141.56:39338/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/jsr305-3.0.0.jar,spark://192.168.141.56:39338/jars/rapids-4-spark_2.12-24.06.1.jar,spark://192.168.141.56:39338/jars/hadoop-client-api-3.3.4.jar,spark://192.168.141.56:39338/jars/slf4j-api-2.0.7.jar,spark://192.168.141.56:39338/jars/commons-pool2-2.11.1.jar,spark://192.168.141.56:39338/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/commons-logging-1.1.3.jar,spark://192.168.141.56:39338/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39338/jars/lz4-java-1.8.0.jar,spark://192.168.141.56:39338/jars/snappy-java-1.1.10.3.jar\n",
      "spark.app.name = DDP_KBIT_Experiments\n",
      "spark.app.startTime = 1755667529689\n",
      "spark.app.submitTime = 1755667526928\n",
      "spark.default.parallelism = 64\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host = 192.168.141.56\n",
      "spark.driver.port = 39338\n",
      "spark.executor.cores = 4\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id = driver\n",
      "spark.executor.instances = 8\n",
      "spark.executor.memory = 6G\n",
      "spark.executor.resource.gpu.amount = 1\n",
      "spark.executor.resource.gpu.discoveryScript = /opt/spark/conf/getGpusResources.sh\n",
      "spark.files = file:///mnt/data/mnist_pb2.py\n",
      "spark.jars = jars/commons-logging-1.1.3.jar,jars/commons-pool2-2.11.1.jar,jars/hadoop-client-api-3.3.4.jar,jars/hadoop-client-runtime-3.3.4.jar,jars/jsr305-3.0.0.jar,jars/kafka-clients-3.4.1.jar,jars/lz4-java-1.8.0.jar,jars/slf4j-api-2.0.7.jar,jars/snappy-java-1.1.10.3.jar,jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.master = spark://spark-master-service:7077\n",
      "spark.plugins = com.nvidia.spark.SQLPlugin\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.driver.user.timezone = Z\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rapids.driver.user.timezone = Z\n",
      "spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rdd.compress = True\n",
      "spark.repl.local.jars = file:///mnt/data/jars/commons-logging-1.1.3.jar,file:///mnt/data/jars/commons-pool2-2.11.1.jar,file:///mnt/data/jars/hadoop-client-api-3.3.4.jar,file:///mnt/data/jars/hadoop-client-runtime-3.3.4.jar,file:///mnt/data/jars/jsr305-3.0.0.jar,file:///mnt/data/jars/kafka-clients-3.4.1.jar,file:///mnt/data/jars/lz4-java-1.8.0.jar,file:///mnt/data/jars/slf4j-api-2.0.7.jar,file:///mnt/data/jars/snappy-java-1.1.10.3.jar,file:///mnt/data/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.sql.extensions = com.nvidia.spark.rapids.SQLExecPlugin,com.nvidia.spark.udf.Plugin,com.nvidia.spark.rapids.optimizer.SQLOptimizerPlugin\n",
      "spark.sql.shuffle.partitions = 64\n",
      "spark.submit.deployMode = client\n",
      "spark.submit.pyFiles = mnist_pb2.py\n",
      "spark.task.resource.gpu.amount = 1\n",
      "spark.ui.showConsoleProgress = true\n",
      "❌ Multiple experiments failed: run_multiple_experiments() got an unexpected keyword argument 'iterations'\n"
     ]
    }
   ],
   "source": [
    "# Multiple experiments with statistical analysis\n",
    "print(\"🧪 Running multiple experiments...\")\n",
    "args.experiment_type = \"multiple\"\n",
    "args.iterations = 5  # You can change this number\n",
    "\n",
    "try:\n",
    "    run_experiment_mode(args)\n",
    "    print(f\"✅ {args.iterations} experiments completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Multiple experiments failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Configuration\n",
    "\n",
    "Modify configuration parameters for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'custom_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Save custom configuration\u001b[39;00m\n\u001b[1;32m     23\u001b[0m custom_config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcustom_config_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     25\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(custom_config, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Update args to use custom config\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'custom_config.json'"
     ]
    }
   ],
   "source": [
    "# Customize configuration\n",
    "custom_config = {\n",
    "    \"spark_config\": {\n",
    "        \"master\": \"local[*]\",\n",
    "        \"app_name\": \"DDP_KBIT_Custom\",\n",
    "        \"executor_instances\": 4,\n",
    "        \"executor_cores\": 2,\n",
    "        \"executor_memory\": \"8g\"\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.0001\n",
    "    },\n",
    "    \"data_config\": {\n",
    "        \"kafka_servers\": [\"localhost:9092\"],\n",
    "        \"topic\": \"custom_topic\",\n",
    "        \"batch_size\": 64\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save custom configuration\n",
    "custom_config_path = \"custom_config.json\"\n",
    "with open(custom_config_path, \"w\") as f:\n",
    "    json.dump(custom_config, f, indent=2)\n",
    "\n",
    "# Update args to use custom config\n",
    "args.config_path = custom_config_path\n",
    "\n",
    "print(f\"✓ Custom configuration saved to: {custom_config_path}\")\n",
    "print(\"\\nCustom configuration:\")\n",
    "print(json.dumps(custom_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for notebook usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train(distributed=False, config_path=\"sample_config.json\"):\n",
    "    \"\"\"Quick training function for easy execution.\"\"\"\n",
    "    args.distributed = distributed\n",
    "    args.config_path = config_path\n",
    "    \n",
    "    print(f\"🚀 Quick training - Distributed: {distributed}\")\n",
    "    try:\n",
    "        run_training_mode(args)\n",
    "        print(\"✅ Training completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "\n",
    "def quick_experiment(experiment_type=\"single\", iterations=3):\n",
    "    \"\"\"Quick experiment function for easy execution.\"\"\"\n",
    "    args.experiment_type = experiment_type\n",
    "    args.iterations = iterations\n",
    "    \n",
    "    print(f\"🧪 Quick experiment - Type: {experiment_type}, Iterations: {iterations}\")\n",
    "    try:\n",
    "        run_experiment_mode(args)\n",
    "        print(\"✅ Experiment completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Experiment failed: {e}\")\n",
    "\n",
    "print(\"✓ Utility functions loaded!\")\n",
    "print(\"\\nUse these functions for quick execution:\")\n",
    "print(\"- quick_train(distributed=False)\")\n",
    "print(\"- quick_experiment(experiment_type='multiple', iterations=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Execution Examples\n",
    "\n",
    "Use the utility functions for quick execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Quick single training\n",
    "# quick_train()\n",
    "\n",
    "# Example: Quick multiple experiments\n",
    "# quick_experiment(experiment_type=\"multiple\", iterations=3)\n",
    "\n",
    "print(\"💡 Uncomment the lines above to run quick examples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zl156b5fcv",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files in current directory:\")\n",
    "for f in os.listdir('.'):\n",
    "    print(f\"  {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
