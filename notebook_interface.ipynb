{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP_KBIT Jupyter Notebook Interface\n",
    "\n",
    "This notebook provides a simple interface to run the DDP_KBIT distributed deep learning system without using command line arguments. It wraps the existing `main.py` functionality for easy experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî (Îß§Î≤à Ïã§Ìñâ ÌïÑÏöî)\n",
    "\n",
    "ÏïÑÎûò ÏÖÄÏùÑ Îß§ ÏÑ∏ÏÖòÎßàÎã§ Í∞ÄÏû• Î®ºÏ†Ä Ïã§ÌñâÌïòÏó¨ Î°úÏª¨ Î™®ÎìàÏóê Ïó∞Í≤∞ÌïòÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found DDP_KBIT at: /mnt/data/DDP_KBIT\n",
      "‚úì Successfully imported DDP_KBIT modules\n",
      "üéâ DDP_KBIT notebook interface ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def find_ddp_kbit_path():\n",
    "    \"\"\"Find DDP_KBIT directory in common locations.\"\"\"\n",
    "    current_dir = Path.cwd()\n",
    "    possible_paths = [\n",
    "        current_dir,\n",
    "        current_dir.parent,\n",
    "        Path(\"/mnt/data/DDP_KBIT\"),\n",
    "        Path(r\"D:\\Nextcloud3\\kbit\\DDP_KBIT\"),\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if (path / \"main.py\").exists():\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "def setup_module_path():\n",
    "    \"\"\"Setup Python path for DDP_KBIT modules.\"\"\"\n",
    "    ddp_path = find_ddp_kbit_path()\n",
    "    if not ddp_path:\n",
    "        print(\"‚ùå Could not find DDP_KBIT directory\")\n",
    "        return None\n",
    "    \n",
    "    # Add both the directory and its parent to handle package imports\n",
    "    parent_path = str(ddp_path.parent)\n",
    "    ddp_str = str(ddp_path)\n",
    "    \n",
    "    if parent_path not in sys.path:\n",
    "        sys.path.insert(0, parent_path)\n",
    "    if ddp_str not in sys.path:\n",
    "        sys.path.insert(0, ddp_str)\n",
    "        \n",
    "    print(f\"‚úì Found DDP_KBIT at: {ddp_path}\")\n",
    "    return ddp_path\n",
    "\n",
    "# Initialize DDP_KBIT\n",
    "if setup_module_path():\n",
    "    try:\n",
    "        from main import (\n",
    "            setup_logging, \n",
    "            load_external_config,\n",
    "            run_training_mode,\n",
    "            run_experiment_mode, \n",
    "            create_sample_config\n",
    "        )\n",
    "        print(\"‚úì Successfully imported DDP_KBIT modules\")\n",
    "        print(\"üéâ DDP_KBIT notebook interface ready!\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Error importing DDP_KBIT modules: {e}\")\n",
    "        print(\"‚ö†Ô∏è  DDP_KBIT setup incomplete. Some features may not work.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DDP_KBIT setup incomplete. Some features may not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration setup complete\n",
      "Config path: sample_config.json\n",
      "Distributed: False\n",
      "Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "setup_logging(\"INFO\")\n",
    "\n",
    "# Create a mock args object to simulate command line arguments\n",
    "class NotebookArgs:\n",
    "    def __init__(self):\n",
    "        self.config_path = \"sample_config.json\"\n",
    "        self.distributed = False\n",
    "        self.experiment_type = \"single\"\n",
    "        self.iterations = 3\n",
    "        self.log_level = \"INFO\"\n",
    "\n",
    "# Initialize default arguments\n",
    "args = NotebookArgs()\n",
    "\n",
    "print(\"‚úì Configuration setup complete\")\n",
    "print(f\"Config path: {args.config_path}\")\n",
    "print(f\"Distributed: {args.distributed}\")\n",
    "print(f\"Iterations: {args.iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Configuration (Run this first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample_config.json - customize this file for your needs.\n",
      "‚úì Sample configuration created!\n",
      "\n",
      "Current configuration:\n",
      "{\n",
      "  \"training_config\": {\n",
      "    \"base_model_type\": \"NeuralNetwork\",\n",
      "    \"optimizer_class\": \"torch.optim.adam.Adam\",\n",
      "    \"optimizer_params\": {\n",
      "      \"lr\": 0.001\n",
      "    },\n",
      "    \"loss_fn\": \"torch.nn.modules.loss.CrossEntropyLoss\",\n",
      "    \"perform_validation\": true,\n",
      "    \"num_epochs\": 1,\n",
      "    \"batch_size\": 32,\n",
      "    \"metrics\": {\n",
      "      \"loss\": \"Loss\",\n",
      "      \"accuracy\": \"Accuracy\"\n",
      "    }\n",
      "  },\n",
      "  \"mongo_config\": {\n",
      "    \"connection_id\": \"my-mongo-1\",\n",
      "    \"mongo_database\": \"kbit-db\",\n",
      "    \"collection\": \"mnist_train_avro\"\n",
      "  },\n",
      "  \"kafka_config\": {\n",
      "    \"bootstrap_servers\": [\n",
      "      \"155.230.35.200:32100\",\n",
      "      \"155.230.35.213:32100\",\n",
      "      \"155.230.35.215:32100\"\n",
      "    ],\n",
      "    \"data_load_topic\": \"kbit-p3r1\"\n",
      "  },\n",
      "  \"data_loader_config\": {\n",
      "    \"data_loader_type\": \"kafka\",\n",
      "    \"local_data_path\": \"/root/processed_mnist\",\n",
      "    \"offsets_data\": [\n",
      "      \"0:0:19999\",\n",
      "      \"1:0:19999\",\n",
      "      \"2:0:19999\"\n",
      "    ],\n",
      "    \"offsets_data_topic\": \"my-topic-3\",\n",
      "    \"api_config\": {\n",
      "      \"base_url\": \"http://155.230.36.25:3001\",\n",
      "      \"endpoint\": \"data/export\",\n",
      "      \"params\": {\n",
      "        \"connection_id\": \"my-mongo-1\",\n",
      "        \"mongo_database\": \"kbit-db\",\n",
      "        \"collection\": \"mnist_train_avro\",\n",
      "        \"kafka_brokers\": \"155.230.35.200:32100,155.230.35.213:32100,155.230.35.215:32100\",\n",
      "        \"send_topic\": \"kbit-p3r1\"\n",
      "      }\n",
      "    },\n",
      "    \"dataset_split_config\": [\n",
      "      {\n",
      "        \"rate\": 0.85715\n",
      "      },\n",
      "      {\n",
      "        \"rate\": 0.071425\n",
      "      },\n",
      "      {\n",
      "        \"rate\": 0.071425\n",
      "      }\n",
      "    ],\n",
      "    \"consumer_params\": {\n",
      "      \"bootstrap_servers\": [\n",
      "        \"155.230.35.200:32100\",\n",
      "        \"155.230.35.213:32100\",\n",
      "        \"155.230.35.215:32100\"\n",
      "      ]\n",
      "    },\n",
      "    \"payload_config\": {\n",
      "      \"message_format\": \"none\",\n",
      "      \"data_field\": \"data\",\n",
      "      \"label_field\": \"label\",\n",
      "      \"transform_data_fn\": \"transform_mongodb_image\",\n",
      "      \"transform_label_fn\": null\n",
      "    }\n",
      "  },\n",
      "  \"payload_config\": {\n",
      "    \"message_format\": \"none\",\n",
      "    \"data_field\": \"data\",\n",
      "    \"label_field\": \"label\",\n",
      "    \"transform_data_fn\": \"transform_mongodb_image\",\n",
      "    \"transform_label_fn\": null\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"generated_from\": \"DDP_KBIT.config modules\",\n",
      "    \"description\": \"Sample configuration dynamically generated from existing config modules\",\n",
      "    \"usage_note\": \"This config uses the same settings as defined in the original notebook cell 24\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a sample configuration file\n",
    "create_sample_config()\n",
    "print(\"‚úì Sample configuration created!\")\n",
    "\n",
    "# Display the configuration\n",
    "if os.path.exists(\"sample_config.json\"):\n",
    "    with open(\"sample_config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"\\nCurrent configuration:\")\n",
    "    print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode\n",
    "\n",
    "Run single node or distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 05:44:52 - root - INFO - Starting training mode...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting single node training...\n",
      "FILES IN THIS DIRECTORY\n",
      "['DDP_KBIT', 'jars', 'config.json', 'mnist_pb2.py', 'spark_DL_checkpoints']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/20 05:44:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/20 05:44:54 WARN ResourceUtils: The configuration of cores (exec = 5 task = 1, runnable tasks = 5) will result in wasted resources due to resource gpu limiting the number of runnable tasks per executor to: 1. Please adjust your configuration.\n",
      "25/08/20 05:44:55 WARN RapidsPluginUtils: RAPIDS Accelerator 24.06.1 using cudf 24.06.0, private revision 755b4dd03c753cacb7d141f3b3c8ff9f83888b69\n",
      "25/08/20 05:44:55 WARN RapidsPluginUtils: spark.rapids.sql.multiThreadedRead.numThreads is set to 20.\n",
      "25/08/20 05:44:55 WARN RapidsPluginUtils: The current setting of spark.task.resource.gpu.amount (1.0) is not ideal to get the best performance from the RAPIDS Accelerator plugin. It's recommended to be 1/{executor core count} unless you have a special use case.\n",
      "25/08/20 05:44:55 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "25/08/20 05:44:55 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `NOT_ON_GPU`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n",
      "2025-08-20 05:44:55 - root - ERROR - Training failed: main_fn() missing 3 required positional arguments: 'training_config', 'kafka_config', and 'data_loader_config'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Spark configuration:\n",
      "spark.app.id = app-20250820054455-0022\n",
      "spark.app.initial.file.urls = spark://192.168.141.56:39337/files/mnist_pb2.py\n",
      "spark.app.initial.jar.urls = spark://192.168.141.56:39337/jars/jsr305-3.0.0.jar,spark://192.168.141.56:39337/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39337/jars/commons-pool2-2.11.1.jar,spark://192.168.141.56:39337/jars/rapids-4-spark_2.12-24.06.1.jar,spark://192.168.141.56:39337/jars/commons-logging-1.1.3.jar,spark://192.168.141.56:39337/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39337/jars/hadoop-client-runtime-3.3.4.jar,spark://192.168.141.56:39337/jars/lz4-java-1.8.0.jar,spark://192.168.141.56:39337/jars/kafka-clients-3.4.1.jar,spark://192.168.141.56:39337/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,spark://192.168.141.56:39337/jars/slf4j-api-2.0.7.jar,spark://192.168.141.56:39337/jars/snappy-java-1.1.10.3.jar,spark://192.168.141.56:39337/jars/hadoop-client-api-3.3.4.jar\n",
      "spark.app.name = DDP_KBIT_Training\n",
      "spark.app.startTime = 1755668694543\n",
      "spark.app.submitTime = 1755668694417\n",
      "spark.defaul.parallelism = 30\n",
      "spark.default.parallelism = 64\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host = 192.168.141.56\n",
      "spark.driver.port = 39337\n",
      "spark.executor.cores = 5\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id = driver\n",
      "spark.executor.instances = 3\n",
      "spark.executor.memory = 24g\n",
      "spark.executor.resource.gpu.amount = 1\n",
      "spark.executor.resource.gpu.discoveryScript = /opt/spark/conf/getGpusResources.sh\n",
      "spark.files = file:///mnt/data/mnist_pb2.py\n",
      "spark.jars = jars/commons-logging-1.1.3.jar,jars/commons-pool2-2.11.1.jar,jars/hadoop-client-api-3.3.4.jar,jars/hadoop-client-runtime-3.3.4.jar,jars/jsr305-3.0.0.jar,jars/kafka-clients-3.4.1.jar,jars/lz4-java-1.8.0.jar,jars/slf4j-api-2.0.7.jar,jars/snappy-java-1.1.10.3.jar,jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.master = spark://spark-master-service:7077\n",
      "spark.plugins = com.nvidia.spark.SQLPlugin\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.driver.user.timezone = Z\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rapids.driver.user.timezone = Z\n",
      "spark.rapids.memory.gpu.minAllocFraction = 0.1\n",
      "spark.rapids.memory.pinnedPool.size = 2g\n",
      "spark.rapids.sql.concurrentGpuTasks = 1\n",
      "spark.rapids.sql.multiThreadedRead.numThreads = 20\n",
      "spark.rdd.compress = True\n",
      "spark.repl.local.jars = file:///mnt/data/jars/commons-logging-1.1.3.jar,file:///mnt/data/jars/commons-pool2-2.11.1.jar,file:///mnt/data/jars/hadoop-client-api-3.3.4.jar,file:///mnt/data/jars/hadoop-client-runtime-3.3.4.jar,file:///mnt/data/jars/jsr305-3.0.0.jar,file:///mnt/data/jars/kafka-clients-3.4.1.jar,file:///mnt/data/jars/lz4-java-1.8.0.jar,file:///mnt/data/jars/slf4j-api-2.0.7.jar,file:///mnt/data/jars/snappy-java-1.1.10.3.jar,file:///mnt/data/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,file:///mnt/data/jars/rapids-4-spark_2.12-24.06.1.jar\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.sql.extensions = com.nvidia.spark.rapids.SQLExecPlugin,com.nvidia.spark.udf.Plugin,com.nvidia.spark.rapids.optimizer.SQLOptimizerPlugin\n",
      "spark.sql.shuffle.partitions = 30\n",
      "spark.submit.deployMode = client\n",
      "spark.submit.pyFiles = mnist_pb2.py\n",
      "spark.task.resource.gpu.amount = 1\n",
      "spark.ui.showConsoleProgress = true\n",
      "‚ùå Training failed: main_fn() missing 3 required positional arguments: 'training_config', 'kafka_config', and 'data_loader_config'\n"
     ]
    }
   ],
   "source": [
    "# Single node training\n",
    "print(\"üöÄ Starting single node training...\")\n",
    "args.distributed = False\n",
    "\n",
    "try:\n",
    "    run_training_mode(args)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training (uncomment to run)\n",
    "# print(\"üöÄ Starting distributed training...\")\n",
    "# args.distributed = True\n",
    "\n",
    "# try:\n",
    "#     run_training_mode(args)\n",
    "#     print(\"‚úÖ Distributed training completed successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Distributed training failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Mode\n",
    "\n",
    "Run single experiments or multiple iterations with statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single experiment\n",
    "print(\"üß™ Running single experiment...\")\n",
    "args.experiment_type = \"single\"\n",
    "\n",
    "try:\n",
    "    run_experiment_mode(args)\n",
    "    print(\"‚úÖ Single experiment completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Single experiment failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple experiments with statistical analysis\n",
    "print(\"üß™ Running multiple experiments...\")\n",
    "args.experiment_type = \"multiple\"\n",
    "args.iterations = 5  # You can change this number\n",
    "\n",
    "try:\n",
    "    run_experiment_mode(args)\n",
    "    print(f\"‚úÖ {args.iterations} experiments completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Multiple experiments failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Configuration\n",
    "\n",
    "Modify configuration parameters for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize configuration\n",
    "custom_config = {\n",
    "    \"spark_config\": {\n",
    "        \"master\": \"local[*]\",\n",
    "        \"app_name\": \"DDP_KBIT_Custom\",\n",
    "        \"executor_instances\": 4,\n",
    "        \"executor_cores\": 2,\n",
    "        \"executor_memory\": \"8g\"\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.0001\n",
    "    },\n",
    "    \"data_config\": {\n",
    "        \"kafka_servers\": [\"localhost:9092\"],\n",
    "        \"topic\": \"custom_topic\",\n",
    "        \"batch_size\": 64\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save custom configuration\n",
    "custom_config_path = \"custom_config.json\"\n",
    "with open(custom_config_path, \"w\") as f:\n",
    "    json.dump(custom_config, f, indent=2)\n",
    "\n",
    "# Update args to use custom config\n",
    "args.config_path = custom_config_path\n",
    "\n",
    "print(f\"‚úì Custom configuration saved to: {custom_config_path}\")\n",
    "print(\"\\nCustom configuration:\")\n",
    "print(json.dumps(custom_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for notebook usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train(distributed=False, config_path=\"sample_config.json\"):\n",
    "    \"\"\"Quick training function for easy execution.\"\"\"\n",
    "    args.distributed = distributed\n",
    "    args.config_path = config_path\n",
    "    \n",
    "    print(f\"üöÄ Quick training - Distributed: {distributed}\")\n",
    "    try:\n",
    "        run_training_mode(args)\n",
    "        print(\"‚úÖ Training completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "\n",
    "def quick_experiment(experiment_type=\"single\", iterations=3):\n",
    "    \"\"\"Quick experiment function for easy execution.\"\"\"\n",
    "    args.experiment_type = experiment_type\n",
    "    args.iterations = iterations\n",
    "    \n",
    "    print(f\"üß™ Quick experiment - Type: {experiment_type}, Iterations: {iterations}\")\n",
    "    try:\n",
    "        run_experiment_mode(args)\n",
    "        print(\"‚úÖ Experiment completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Experiment failed: {e}\")\n",
    "\n",
    "print(\"‚úì Utility functions loaded!\")\n",
    "print(\"\\nUse these functions for quick execution:\")\n",
    "print(\"- quick_train(distributed=False)\")\n",
    "print(\"- quick_experiment(experiment_type='multiple', iterations=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Execution Examples\n",
    "\n",
    "Use the utility functions for quick execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Quick single training\n",
    "# quick_train()\n",
    "\n",
    "# Example: Quick multiple experiments\n",
    "# quick_experiment(experiment_type=\"multiple\", iterations=3)\n",
    "\n",
    "print(\"üí° Uncomment the lines above to run quick examples!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
